{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPW6/opr6FtvIW3JYsj+VN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnanthecoder/Generative_ai/blob/main/GAI_W4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfrFUFilOpxw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "01.Design a simple ANN architecture with one input and one output layer (no hidden layer). Assume a linear activation function in the output layer. • Write Python code for a backpropagation algorithm with gradient descent optimization to update weights and bias parameters of the ANN model with training data shown in Table 1. • Calculate the mean square error with training and testing data shown in Table 2. • Write Python code that reads the input data [x1, x2, and x3] from the user. Predict the output with deployed ANN model Tabela 1: Training Data x1 x2 x3 y 0.1 0.2 0.3 0.14 0.2 0.3 0.4 0.20 0.3 0.4 0.5 0.26 0.5 0.6 0.7 0.38 0.1 0.3 0.5 0.22 0.2 0.4 0.6 0.28 0.3 0.5 0.7 0.34 0.4 0.6 0.8 0.40 0.5 0.7 0.1 0.22 Tabela 2: Test Data x1 x2 x3 y 0.6 0.7 0.8 0.44 0.7 0.8 0.9 0.50"
      ],
      "metadata": {
        "id": "aYxZZACzO6w1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data_train = np.array([\n",
        "    [0.1, 0.2, 0.3, 0.14],\n",
        "    [0.2, 0.3, 0.4, 0.20],\n",
        "    [0.3, 0.4, 0.5, 0.26],\n",
        "    [0.5, 0.6, 0.7, 0.38],\n",
        "    [0.1, 0.3, 0.5, 0.22],\n",
        "    [0.2, 0.4, 0.6, 0.28],\n",
        "    [0.3, 0.5, 0.7, 0.34],\n",
        "    [0.4, 0.6, 0.8, 0.40],\n",
        "    [0.5, 0.7, 0.1, 0.22]\n",
        "])\n",
        "\n",
        "data_test = np.array([\n",
        "    [0.6, 0.7, 0.8, 0.44],\n",
        "    [0.7, 0.8, 0.9, 0.50]\n",
        "])\n",
        "\n",
        "X_train = data_train[:, :3]\n",
        "y_train = data_train[:, 3]\n",
        "\n",
        "X_test = data_test[:, :3]\n",
        "y_test = data_test[:, 3]\n",
        "\n",
        "weights = np.random.rand(3)\n",
        "bias = np.random.rand(1)[0]\n",
        "learning_rate = 0.01\n",
        "num_epochs = 1000\n",
        "def linear_activation(z):\n",
        "    return z\n",
        "\n",
        "def linear_activation_derivative(z):\n",
        "    return 1\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    y_pred_train = linear_activation(np.dot(X_train, weights) + bias)\n",
        "\n",
        "    error = y_pred_train - y_train\n",
        "    d_weights = np.dot(X_train.T, error) / len(X_train)\n",
        "    d_bias = np.sum(error) / len(X_train)\n",
        "\n",
        "    weights -= learning_rate * d_weights\n",
        "    bias -= learning_rate * d_bias\n",
        "\n",
        "y_pred_train_final = linear_activation(np.dot(X_train, weights) + bias)\n",
        "y_pred_test_final = linear_activation(np.dot(X_test, weights) + bias)\n",
        "\n",
        "mse_train = mean_squared_error(y_train, y_pred_train_final)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test_final)\n",
        "\n",
        "print(\"Final weights:\", weights)\n",
        "print(\"Final bias:\", bias)\n",
        "print(\"Training MSE:\", mse_train)\n",
        "print(\"Testing MSE:\", mse_test)\n",
        "\n",
        "user_input = np.array([float(x) for x in input(\"Enter values for x1, x2, x3 (comma-separated): \").replace(\" \", \"\").split(\",\")])\n",
        "user_prediction = linear_activation(np.dot(user_input, weights) + bias)\n",
        "print(\"Predicted output for input\", user_input, \":\", user_prediction)"
      ],
      "metadata": {
        "id": "sjt75tSWQ_lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "02.Design a simple ANN architecture with one input and one output layer (no hidden layer). Assume a sigmoid activation function shown in the equation 1 in the output layer. f (x) = 1 1 + e−x (1) • Write Python code for a backpropagation algorithm with gradient descent optimization to update weights and bias parameters of the ANN model with training data shown in Table 3.Tabela 3: Training Data x1 x2 x3 y 0.1 0.2 0.3 0.5349 0.2 0.3 0.4 0.5498 0.3 0.4 0.5 0.5646 0.5 0.6 0.7 0.5939 0.1 0.3 0.5 0.5548 0.2 0.4 0.6 0.5695 0.3 0.5 0.7 0.5842 0.4 0.6 0.8 0.5987 0.5 0.7 0.1 0.5548 Tabela 4: Test Data x1 x2 x3 y 0.6 0.7 0.8 0.6083 0.7 0.8 0.9 0.6225 • Calculate the mean square error with training and testing data shown in Table 4. • Write Python code that reads the input data [x1, x2, and x3] from the user. Predict the output with deployed ANN model • Expected learning Outcomes from this assignment related to python – Students are able to understand how backpropagation algorithm helps to update model parameters of ANN – Students are able to write code in python for backpropagation algorithm – Students are able to design architecture of ANN based on problem statement – Students are able to derive mathematical expression for change in weights and bias parameters for different activation functions"
      ],
      "metadata": {
        "id": "mSda8cpjPOpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data_train = np.array([\n",
        "    [0.1, 0.2, 0.3, 0.5349],\n",
        "    [0.2, 0.3, 0.4, 0.5498],\n",
        "    [0.3, 0.4, 0.5, 0.5646],\n",
        "    [0.5, 0.6, 0.7, 0.5939],\n",
        "    [0.1, 0.3, 0.5, 0.5548],\n",
        "    [0.2, 0.4, 0.6, 0.5695],\n",
        "    [0.3, 0.5, 0.7, 0.5842],\n",
        "    [0.4, 0.6, 0.8, 0.5987],\n",
        "    [0.5, 0.7, 0.1, 0.5548]\n",
        "])\n",
        "\n",
        "data_test = np.array([\n",
        "    [0.6, 0.7, 0.8, 0.6083],\n",
        "    [0.7, 0.8, 0.9, 0.6225]\n",
        "])\n",
        "\n",
        "X_train = data_train[:, :3]\n",
        "y_train = data_train[:, 3]\n",
        "X_test = data_test[:, :3]\n",
        "y_test = data_test[:, 3]\n",
        "\n",
        "weights = np.random.rand(3)\n",
        "bias = np.random.rand(1)[0]\n",
        "learning_rate = 0.01\n",
        "num_epochs = 1000\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    z = np.dot(X_train, weights) + bias\n",
        "    y_pred_train = sigmoid(z)\n",
        "\n",
        "    error = y_pred_train - y_train\n",
        "    d_weights = np.dot(X_train.T, error * sigmoid_derivative(y_pred_train)) / len(X_train)\n",
        "    d_bias = np.sum(error * sigmoid_derivative(y_pred_train)) / len(X_train)\n",
        "\n",
        "    weights -= learning_rate * d_weights\n",
        "    bias -= learning_rate * d_bias\n",
        "\n",
        "y_pred_train_final = sigmoid(np.dot(X_train, weights) + bias)\n",
        "y_pred_test_final = sigmoid(np.dot(X_test, weights) + bias)\n",
        "\n",
        "mse_train = mean_squared_error(y_train, y_pred_train_final)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test_final)\n",
        "\n",
        "print(\"Final weights:\", weights)\n",
        "print(\"Final bias:\", bias)\n",
        "print(\"Training MSE:\", mse_train)\n",
        "print(\"Testing MSE:\", mse_test)\n",
        "\n",
        "user_input = np.array([float(x) for x in input(\"Enter values for x1, x2, x3 (comma-separated): \").replace(\" \", \"\").split(\",\")])\n",
        "user_prediction = sigmoid(np.dot(user_input, weights) + bias)\n",
        "print(\"Predicted output for input\", user_input, \":\", user_prediction)"
      ],
      "metadata": {
        "id": "-IWNAJKxPaRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df21c442-7f80-426a-b007-2ab31c74fdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final weights: [ 0.35103378  0.55678853 -0.0758337 ]\n",
            "Final bias: 0.5377638797817939\n",
            "Training MSE: 0.017981531527452257\n",
            "Testing MSE: 0.019136793367468014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZDEGlIcsQeWp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}